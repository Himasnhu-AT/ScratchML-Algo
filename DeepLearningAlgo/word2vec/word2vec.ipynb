{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "898b852e",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "Word2vec is a vector representation of text which has context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5328662",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:53.199103Z",
     "start_time": "2022-04-27T13:32:45.793371Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6071ff9",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71425658",
   "metadata": {},
   "source": [
    "This is just a small Text paragraph from Wikipedia about Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f10fa281",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:53.215070Z",
     "start_time": "2022-04-27T13:32:53.201068Z"
    }
   },
   "outputs": [],
   "source": [
    "text = '''Machine learning is the study of computer algorithms that \\\n",
    "improve automatically through experience. It is seen as a \\\n",
    "subset of artificial intelligence. Machine learning algorithms \\\n",
    "build a mathematical model based on sample data, known as \\\n",
    "training data, in order to make predictions or decisions without \\\n",
    "being explicitly programmed to do so. Machine learning algorithms \\\n",
    "are used in a wide variety of applications, such as email filtering \\\n",
    "and computer vision, where it is difficult or infeasible to develop \\\n",
    "conventional algorithms to perform the needed tasks.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6206e58a",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09316952",
   "metadata": {},
   "source": [
    "Since we can’t feed raw string texts into our model, we will need to preprocess this text. The first step, as is the approach taken in many NLP tasks, is to tokenize the text, i.e. splitting the text up into smaller units like words, getting rid of punctuations, and so on. Here is a function that does this trick using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c3e6184",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:53.231113Z",
     "start_time": "2022-04-27T13:32:53.217097Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a6790",
   "metadata": {},
   "source": [
    "Removed stop words from our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9921b553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning study computer algorithms improve automatically experience. seen subset artificial intelligence. Machine learning algorithms build mathematical model based sample data, known training data, order make predictions decisions explicitly programmed so. Machine learning algorithms used wide variety applications, email filtering computer vision, difficult infeasible develop conventional algorithms perform needed tasks.\n",
      "Old length:  556\n",
      "New length:  433\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "words = [word for word in text.split() if word.lower() not in ENGLISH_STOP_WORDS]\n",
    "new_text = \" \".join(words)\n",
    "print(new_text)\n",
    "print(\"Old length: \", len(text))\n",
    "print(\"New length: \", len(new_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf919781",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:53.247068Z",
     "start_time": "2022-04-27T13:32:53.234070Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = tokenize(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3142f7ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:53.263068Z",
     "start_time": "2022-04-27T13:32:53.248068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine',\n",
       " 'learning',\n",
       " 'study',\n",
       " 'computer',\n",
       " 'algorithms',\n",
       " 'improve',\n",
       " 'automatically',\n",
       " 'experience',\n",
       " 'seen',\n",
       " 'subset']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ff18e0",
   "metadata": {},
   "source": [
    "Another useful operation is to create a map between tokens and indices, and vice versa. In a sense, we are creating a lookup table that allows us to easily convert from words to indices, and indices to words. This will be particularly useful later on when we perform operations such as one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f2099a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:53.279139Z",
     "start_time": "2022-04-27T13:32:53.265072Z"
    }
   },
   "outputs": [],
   "source": [
    "def mapping(tokens):\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    \n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "    \n",
    "    return word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ce229f",
   "metadata": {},
   "source": [
    "Let’s check if the word-to-index and index-to-word maps have successfully been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a104f4ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:53.294714Z",
     "start_time": "2022-04-27T13:32:53.281670Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 0,\n",
       " 'vision': 1,\n",
       " 'order': 2,\n",
       " 'training': 3,\n",
       " 'infeasible': 4,\n",
       " 'decisions': 5,\n",
       " 'data': 6,\n",
       " 'so': 7,\n",
       " 'applications': 8,\n",
       " 'develop': 9,\n",
       " 'automatically': 10,\n",
       " 'build': 11,\n",
       " 'computer': 12,\n",
       " 'programmed': 13,\n",
       " 'seen': 14,\n",
       " 'variety': 15,\n",
       " 'improve': 16,\n",
       " 'sample': 17,\n",
       " 'used': 18,\n",
       " 'intelligence': 19,\n",
       " 'known': 20,\n",
       " 'filtering': 21,\n",
       " 'perform': 22,\n",
       " 'algorithms': 23,\n",
       " 'tasks': 24,\n",
       " 'artificial': 25,\n",
       " 'based': 26,\n",
       " 'experience': 27,\n",
       " 'mathematical': 28,\n",
       " 'explicitly': 29,\n",
       " 'needed': 30,\n",
       " 'predictions': 31,\n",
       " 'conventional': 32,\n",
       " 'email': 33,\n",
       " 'study': 34,\n",
       " 'make': 35,\n",
       " 'subset': 36,\n",
       " 'learning': 37,\n",
       " 'machine': 38,\n",
       " 'wide': 39,\n",
       " 'difficult': 40}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id, id_to_word = mapping(tokens)\n",
    "word_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e48d2",
   "metadata": {},
   "source": [
    "As we can see, the lookup table is a dictionary object containing the relationship between words and ids. Note that each entry in this lookup table is a token created using the tokenize() function we defined earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3d171f",
   "metadata": {},
   "source": [
    "### Generating Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a13fa",
   "metadata": {},
   "source": [
    "* Now that we have tokenized the text and created lookup tables, we can now proceed to generating the actual training data, which are going to take the form of matrices. \n",
    "* Since tokens are still in the form of strings, we need to encode them numerically using one-hot vectorization. \n",
    "* We also need to generate a bundle of input and target values, as this is a supervised learning technique.\n",
    "* We loop through each word (or token) in the sentence. In each loop, we look at words to the left and right of the input word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd30994",
   "metadata": {},
   "source": [
    "We basically iterate over the tokenized data and generate pairs. One technicality here is that, for the first and last few tokens, it may not be possible to obtain words to the left or right of that input token. In those cases, we simply don’t consider these word pairs and look at only what is feasible without causing IndexErrors. Also note that we create X and y separately instead of putting them in tuple form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a05bd005",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:53.310666Z",
     "start_time": "2022-04-27T13:32:53.297661Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window):\n",
    "    X = []\n",
    "    y = []\n",
    "    n_tokens = len(tokens)\n",
    "    \n",
    "    for i in range(n_tokens):\n",
    "        idx = concat(\n",
    "            range(max(0, i - window), i), \n",
    "            range(i, min(n_tokens, i + window + 1))\n",
    "        )\n",
    "        for j in idx:\n",
    "            if i == j:\n",
    "                continue\n",
    "            X.append(one_hot_encode(word_to_id[tokens[i]], len(word_to_id)))\n",
    "            y.append(one_hot_encode(word_to_id[tokens[j]], len(word_to_id)))\n",
    "    \n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c57f2",
   "metadata": {},
   "source": [
    "Below is the definition for concat, an auxiliary function we used above to combine two range() objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "501965f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:53.326658Z",
     "start_time": "2022-04-27T13:32:53.313662Z"
    }
   },
   "outputs": [],
   "source": [
    "def concat(*iterables):\n",
    "    for iterable in iterables:\n",
    "        yield from iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036529e2",
   "metadata": {},
   "source": [
    "Also, here is the code we use to one-hot vectorize tokens. This process is necessary in order to represent each token as a vector, which can then be stacked to create the matrices X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24b71a87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:53.342695Z",
     "start_time": "2022-04-27T13:32:53.331667Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(id, vocab_size):\n",
    "    res = [0] * vocab_size\n",
    "    res[id] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8012b0fb",
   "metadata": {},
   "source": [
    "Finally, let’s generate some training data with a window size of two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5556878",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:53.358696Z",
     "start_time": "2022-04-27T13:32:53.344665Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = generate_training_data(tokens, word_to_id, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecb39a13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:53.374659Z",
     "start_time": "2022-04-27T13:32:53.361670Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194, 41)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1909b610",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:53.390658Z",
     "start_time": "2022-04-27T13:32:53.376661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194, 41)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66990061",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:53.406663Z",
     "start_time": "2022-04-27T13:32:53.392661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59c4ba69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:53.422663Z",
     "start_time": "2022-04-27T13:32:53.411667Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = tf.constant(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield tf.gather(features, j), tf.gather(labels, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "449db338",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:54.216693Z",
     "start_time": "2022-04-27T13:32:53.426668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n",
      "tf.Tensor(\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]], shape=(10, 41), dtype=int64) \n",
      " tf.Tensor(\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 0 0]], shape=(10, 41), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 00:58:49.540402: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-10-14 00:58:49.540670: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "for X, y in data_iter(batch_size, X, y):\n",
    "    print(X, '\\n', y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8837f42e",
   "metadata": {},
   "source": [
    "### Initializing Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca31753",
   "metadata": {},
   "source": [
    "W1 and W2 are two vectors that will be used for the embedding layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b11f580",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:54.232657Z",
     "start_time": "2022-04-27T13:32:54.218659Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_id)\n",
    "num_hiddens = 10 # Number of dimension\n",
    "\n",
    "# First hidden layer\n",
    "W1 = tf.Variable(tf.random.normal(shape=(vocab_size, num_hiddens), mean=0, stddev=0.01))\n",
    "\n",
    "# Second hidden layer\n",
    "W2 = tf.Variable(tf.random.normal(shape=(num_hiddens, vocab_size), mean=0, stddev=0.01))\n",
    "\n",
    "params = [W1, W2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2d29f5",
   "metadata": {},
   "source": [
    "### Softmax Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea05a7",
   "metadata": {},
   "source": [
    "We will use Softmax Activation.\n",
    "When the output is passed into this layer, it is converted into probability vectors whose elements sum up to one. \n",
    "\n",
    "This final output can be considered as context predictions, i.e. which words are likely to be in the window vicinity of the input word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6386de04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:54.248695Z",
     "start_time": "2022-04-27T13:32:54.234660Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = tf.exp(X)\n",
    "    partition = tf.reduce_sum(X_exp, 1, keepdims=True)\n",
    "    return X_exp / partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85edb61e",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bde7169",
   "metadata": {},
   "source": [
    "Coding the forward propagation process simply amounts to transcribing the three matrix multiplication into NumPy code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66a493b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:54.264159Z",
     "start_time": "2022-04-27T13:32:54.251660Z"
    }
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = tf.reshape(tf.cast(X, dtype=tf.float32), (-1, vocab_size))\n",
    "    a1 = tf.matmul(X, W1)\n",
    "    a2 = tf.matmul(a1, W2)\n",
    "    return softmax(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64d0031",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ac1f9",
   "metadata": {},
   "source": [
    "Loss function used here is Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99a040c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:54.279673Z",
     "start_time": "2022-04-27T13:32:54.266145Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return -tf.math.log(tf.boolean_mask(y_hat, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9089d775",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d26e65c",
   "metadata": {},
   "source": [
    "Here we will be using Stochastic gradient descent as our optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cac38e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:54.295794Z",
     "start_time": "2022-04-27T13:32:54.281611Z"
    }
   },
   "outputs": [],
   "source": [
    "def sgd(params, grads, lr, batch_size):\n",
    "    \"\"\"Minibatch stochastic gradient descent.\n",
    "    Defined in :numref:`sec_linear_scratch`\"\"\"\n",
    "    for param, grad in zip(params, grads):\n",
    "        param.assign_sub(lr*grad/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5ff428",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ba720b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:54.390605Z",
     "start_time": "2022-04-27T13:32:54.297734Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "lr = 0.1\n",
    "loss = cross_entropy\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, X, y):\n",
    "        # Compute gradients and update parameters\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "        # Compute gradient on l with respect to [`w`, `b`]\n",
    "        grads = tape.gradient(l, params)\n",
    "        # Update parameters using their gradient\n",
    "        sgd(params, grads, lr, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3395597",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e91cc76",
   "metadata": {},
   "source": [
    "In this we will be testing our model with a word \"learning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fb9a55c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:54.405475Z",
     "start_time": "2022-04-27T13:32:54.392473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample\n",
      "known\n",
      "improve\n",
      "build\n",
      "so\n",
      "data\n",
      "algorithms\n",
      "difficult\n",
      "training\n",
      "tasks\n",
      "infeasible\n",
      "needed\n",
      "automatically\n",
      "email\n",
      "explicitly\n",
      "variety\n",
      "learning\n",
      "used\n",
      "seen\n",
      "subset\n",
      "study\n",
      "machine\n",
      "make\n",
      "filtering\n",
      "mathematical\n",
      "vision\n",
      "computer\n",
      "artificial\n",
      "applications\n",
      "perform\n",
      "conventional\n",
      "predictions\n",
      "wide\n",
      "decisions\n",
      "based\n",
      "programmed\n",
      "experience\n",
      "develop\n",
      "model\n",
      "order\n",
      "intelligence\n"
     ]
    }
   ],
   "source": [
    "learning = one_hot_encode(word_to_id[\"learning\"], len(word_to_id))\n",
    "result = net([learning])[0]\n",
    "\n",
    "for word in (id_to_word[id] for id in np.argsort(result)[::-1]):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b27ec",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16badb82",
   "metadata": {},
   "source": [
    "The key behind word embeddings is that the rows of the first weight matrix is effectively a dense representation of one-hot encoded vectors each corresponding to various tokens in the text dataset. In our example, therefore, the embedding can simply be obtained by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e87e3d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:54.421472Z",
     "start_time": "2022-04-27T13:32:54.407476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(41, 10) dtype=float32, numpy=\n",
       "array([[ 4.95584542e-03, -6.71986071e-03, -5.92995016e-03,\n",
       "        -6.67822128e-03, -4.51740436e-03,  8.82991962e-03,\n",
       "        -2.54769693e-03, -1.92553527e-03, -1.20385168e-02,\n",
       "         2.48499238e-03],\n",
       "       [ 5.92960091e-03, -1.06932840e-03,  7.76873948e-03,\n",
       "        -1.37154907e-02, -1.62698328e-02,  5.19725541e-03,\n",
       "         4.72769985e-04,  8.68969969e-03,  1.11908894e-02,\n",
       "         1.39254611e-02],\n",
       "       [-3.11099156e-03,  1.12368008e-02,  1.72610674e-02,\n",
       "        -1.97626781e-02,  1.21495221e-02,  1.13645811e-02,\n",
       "         6.05434040e-03,  1.96958939e-03, -7.07712956e-03,\n",
       "         1.57115818e-03],\n",
       "       [ 2.85598799e-03, -5.37711428e-03,  6.09529903e-03,\n",
       "        -7.15362420e-03,  5.26194880e-03,  1.22330440e-02,\n",
       "         2.59799766e-03, -1.20184645e-02, -5.05101774e-03,\n",
       "         1.97129454e-02],\n",
       "       [ 1.69306935e-03,  4.55578929e-03, -1.04620565e-04,\n",
       "         6.15315186e-03,  2.42418591e-02, -1.48237748e-02,\n",
       "         6.77100988e-03,  1.41111119e-02,  2.50103232e-03,\n",
       "        -2.42741872e-03],\n",
       "       [ 9.22777317e-03, -1.12675447e-02, -2.64831586e-03,\n",
       "         3.89802619e-03, -5.24687860e-03, -2.24130116e-02,\n",
       "         1.08898301e-02,  1.15579122e-03,  6.29408658e-03,\n",
       "        -3.18957260e-03],\n",
       "       [ 7.07244175e-03,  2.47988477e-03, -2.74059060e-03,\n",
       "        -7.86502101e-03,  2.22898540e-04, -9.97635443e-03,\n",
       "         1.74768735e-02, -7.60921743e-03,  1.19835446e-02,\n",
       "        -1.00038769e-02],\n",
       "       [-1.26719456e-02, -9.21325665e-03, -3.35142836e-02,\n",
       "        -1.57427620e-02, -7.83536676e-03, -1.89568615e-03,\n",
       "        -1.41824726e-02, -8.23800731e-03,  5.42560685e-03,\n",
       "         1.16886366e-02],\n",
       "       [ 6.66509499e-04,  1.47998089e-03, -9.10091039e-05,\n",
       "        -1.30382236e-02,  9.72371455e-03, -7.16627226e-04,\n",
       "        -4.91732033e-03, -6.25276426e-03, -4.70690575e-04,\n",
       "        -2.68827612e-03],\n",
       "       [ 7.72490306e-03, -7.51548773e-03, -2.84835580e-03,\n",
       "         2.85480171e-03, -6.02034573e-03, -8.47523042e-04,\n",
       "         2.92166648e-03,  7.81851239e-04, -3.67761968e-04,\n",
       "        -3.48041649e-03],\n",
       "       [ 5.28920768e-03,  2.16545304e-03, -2.23282212e-03,\n",
       "         6.55445550e-03,  1.76082202e-03, -4.79303347e-03,\n",
       "        -6.98926393e-04, -6.42125914e-03,  5.60269551e-03,\n",
       "         1.49964755e-02],\n",
       "       [ 1.12849809e-02,  5.35037462e-03,  5.63084008e-03,\n",
       "         3.32982047e-03,  3.49883456e-03, -6.27667131e-03,\n",
       "        -8.53441842e-03,  5.00716176e-03, -1.91537272e-02,\n",
       "         2.33212765e-03],\n",
       "       [-3.40258866e-03,  2.18857881e-02,  1.11405449e-02,\n",
       "        -2.17332714e-03,  1.14946589e-02, -4.78820782e-03,\n",
       "         9.01995134e-03, -4.56639007e-03, -3.61272425e-04,\n",
       "        -4.85177990e-03],\n",
       "       [-1.96277015e-02, -1.13944849e-02, -2.30956636e-02,\n",
       "         5.97347179e-03,  2.44826241e-03, -1.23715298e-02,\n",
       "        -4.34348732e-03,  1.03688403e-03, -9.27234255e-03,\n",
       "         9.28199943e-03],\n",
       "       [ 4.36246954e-03, -1.62030514e-02, -3.32128932e-03,\n",
       "        -2.00025534e-04, -6.66506356e-03, -2.51579471e-03,\n",
       "         1.02689043e-02, -2.24413676e-03,  1.89135429e-02,\n",
       "        -2.06007659e-02],\n",
       "       [-3.64398444e-03, -7.91902281e-03,  1.52015351e-02,\n",
       "        -1.43743207e-04, -1.47471821e-03, -3.56492097e-03,\n",
       "        -1.16332471e-02, -5.74251264e-03, -3.72104091e-03,\n",
       "        -6.13030186e-03],\n",
       "       [ 1.77209161e-03, -1.85161345e-02, -8.48051067e-03,\n",
       "        -1.15089724e-02, -1.80518918e-03,  1.85025588e-03,\n",
       "         1.40985548e-02, -3.32764536e-03,  3.77343316e-03,\n",
       "        -5.91936475e-03],\n",
       "       [ 1.54950703e-02,  8.05215258e-03, -1.22661684e-02,\n",
       "         1.77179975e-03,  7.49048963e-03, -8.80386308e-03,\n",
       "        -1.13971578e-02,  2.35911720e-02,  3.42280767e-03,\n",
       "         1.61720552e-02],\n",
       "       [ 1.09516094e-02, -4.10415418e-03, -1.63466521e-02,\n",
       "         1.11873541e-02,  1.17707858e-02, -1.45931430e-02,\n",
       "        -1.17931319e-02,  1.79035813e-02, -8.34955182e-03,\n",
       "        -1.25589746e-03],\n",
       "       [-4.47809871e-05, -5.74362185e-03, -4.99183452e-03,\n",
       "         4.96696972e-04, -7.36687658e-03,  1.77969001e-02,\n",
       "        -6.99244021e-03, -1.14825135e-02,  3.49538121e-03,\n",
       "         8.09745211e-03],\n",
       "       [ 1.66456755e-02,  1.92603213e-03,  1.51940724e-02,\n",
       "        -3.46598285e-03, -1.13721499e-02,  6.23061741e-03,\n",
       "        -1.77873927e-03, -4.56544198e-03, -1.60530005e-02,\n",
       "        -1.26913115e-02],\n",
       "       [ 2.11675372e-02,  1.71514321e-02,  7.94700813e-03,\n",
       "         3.80957243e-03,  3.97197343e-03, -1.38343964e-02,\n",
       "        -8.67879484e-03, -5.43324137e-03, -1.61956213e-02,\n",
       "         3.66254593e-03],\n",
       "       [-1.29110878e-02, -9.30594839e-03, -1.02127455e-02,\n",
       "         1.22071467e-02,  6.34021964e-03, -1.55508108e-02,\n",
       "        -1.70054939e-03, -7.67123699e-03, -5.53716300e-03,\n",
       "         3.95712070e-03],\n",
       "       [ 1.55679800e-03,  1.24638593e-02,  6.02793507e-03,\n",
       "         1.78187841e-03, -2.72333110e-03, -1.06458711e-02,\n",
       "        -1.56372413e-02, -8.21259338e-03, -9.83471051e-04,\n",
       "        -1.30461901e-02],\n",
       "       [ 9.26512107e-03,  4.26360080e-03, -5.52428828e-04,\n",
       "        -1.62073448e-02, -9.82159935e-03,  3.50351515e-03,\n",
       "        -8.83746427e-03,  6.68277824e-03, -1.39676770e-02,\n",
       "         3.50236078e-04],\n",
       "       [ 5.76159405e-03,  8.37214198e-03, -5.92298387e-03,\n",
       "        -8.71782657e-04,  1.56567455e-03,  1.34554692e-02,\n",
       "        -7.39729684e-03, -8.66995938e-03,  1.23542892e-02,\n",
       "         2.86708083e-02],\n",
       "       [-1.85333993e-02, -9.18171648e-03, -1.21139605e-02,\n",
       "         7.52721727e-03, -1.33283408e-02,  2.35375129e-02,\n",
       "         1.16090160e-02,  1.15424795e-02,  1.11311562e-02,\n",
       "        -2.55128136e-03],\n",
       "       [-4.60820086e-03,  1.16791273e-03, -1.49814680e-03,\n",
       "         2.78852065e-03,  1.90102197e-02,  6.38895296e-03,\n",
       "        -1.00830505e-02, -5.75867807e-03, -1.79480221e-02,\n",
       "         6.15896937e-03],\n",
       "       [-2.60259584e-03,  1.60818629e-03,  1.18153053e-03,\n",
       "        -1.63378264e-03,  6.54802565e-03,  1.82971498e-03,\n",
       "         8.59332271e-03, -2.23629395e-04,  1.01664746e-02,\n",
       "         7.25140003e-03],\n",
       "       [-2.25396035e-03,  1.01468118e-03,  3.89243732e-03,\n",
       "        -4.97757550e-03, -1.06875971e-02, -1.37704909e-02,\n",
       "        -2.49520619e-03, -1.67530347e-02, -1.45963766e-02,\n",
       "         1.71309840e-02],\n",
       "       [-1.25972973e-02,  4.65991115e-03,  1.66116224e-05,\n",
       "        -1.64892040e-02, -4.80749551e-03, -1.03227291e-02,\n",
       "        -8.56337603e-03, -5.12557745e-04,  5.12518454e-03,\n",
       "        -9.06565227e-03],\n",
       "       [ 7.82028027e-03, -5.69187803e-03, -1.31591829e-02,\n",
       "         1.54700479e-03,  3.18577094e-03, -1.40949981e-02,\n",
       "         1.11513305e-02, -4.95324191e-03, -4.75642877e-03,\n",
       "        -1.32555673e-02],\n",
       "       [ 1.96652878e-02, -1.65330749e-02,  3.79940798e-03,\n",
       "        -5.08286944e-03,  7.97268748e-03,  1.06208283e-03,\n",
       "        -1.48898913e-02, -5.86356851e-04, -1.28291571e-03,\n",
       "        -8.88597965e-03],\n",
       "       [ 9.90773737e-03, -7.51860673e-03,  6.31486299e-03,\n",
       "        -7.57583883e-03, -9.37767327e-03, -9.44663584e-03,\n",
       "        -4.89729596e-03,  1.84872393e-02, -1.21343527e-02,\n",
       "        -2.71433149e-03],\n",
       "       [-1.22392150e-02, -1.28402775e-02,  3.90167441e-03,\n",
       "        -4.46698070e-03, -1.30347081e-03, -5.20049129e-03,\n",
       "         1.11699209e-03, -6.49956195e-03,  1.66492146e-02,\n",
       "         2.75171292e-03],\n",
       "       [ 7.53074326e-03, -1.45952161e-02, -5.33415005e-03,\n",
       "        -5.34004159e-03, -1.96598172e-02,  6.01845654e-03,\n",
       "        -4.01124387e-04,  1.20827854e-02,  2.37937598e-03,\n",
       "         7.29589164e-03],\n",
       "       [ 1.05544981e-02,  1.05917193e-02,  1.48607220e-03,\n",
       "        -1.52415261e-02, -9.41553246e-03,  7.14522367e-03,\n",
       "        -9.93704610e-03,  1.05629480e-02,  1.17275375e-03,\n",
       "        -1.78719952e-03],\n",
       "       [ 1.11534130e-02, -6.31334400e-03,  1.17687299e-03,\n",
       "         1.17627205e-02,  1.12606715e-02,  9.11437068e-03,\n",
       "         7.57998461e-03,  1.49197280e-02,  4.87770885e-03,\n",
       "         8.52437969e-03],\n",
       "       [ 3.51905241e-03, -2.63356138e-03,  1.97692425e-03,\n",
       "        -2.58454034e-04,  6.20847056e-03, -1.44476304e-02,\n",
       "         1.02743441e-02, -1.69977720e-03, -7.76966847e-03,\n",
       "         9.28257219e-03],\n",
       "       [ 6.67604245e-03,  1.41409738e-02, -7.32086832e-04,\n",
       "         4.70678788e-03,  3.17983818e-03, -1.65452603e-02,\n",
       "        -6.75340230e-03, -9.34224855e-03, -6.32230949e-05,\n",
       "        -5.14251878e-03],\n",
       "       [ 3.54007259e-03, -1.13985799e-02,  7.79805006e-03,\n",
       "         2.19548792e-02,  1.25749037e-02,  1.36572914e-02,\n",
       "         1.72494128e-02,  3.10515316e-04,  1.51948826e-02,\n",
       "         5.89754572e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14bea45",
   "metadata": {},
   "source": [
    "In particular, what we want is to be able to input a word through a function and receive as output the embedding vector for that given word. Below is a function that implements this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "364e99ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:54.437137Z",
     "start_time": "2022-04-27T13:32:54.424474Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embedding(word):\n",
    "    try:\n",
    "        idx = word_to_id[word]\n",
    "    except KeyError:\n",
    "        print(\"`word` not in corpus\")\n",
    "    \n",
    "    return W1[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25610d2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:32:54.453135Z",
     "start_time": "2022-04-27T13:32:54.438141Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([ 0.00351905, -0.00263356,  0.00197692, -0.00025845,  0.00620847,\n",
       "       -0.01444763,  0.01027434, -0.00169978, -0.00776967,  0.00928257],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embedding(\"machine\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "441c0035c3e70e6a00291a1475037bac16eeba20c63f864469297c06827dc9e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
